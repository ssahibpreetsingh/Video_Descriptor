# Introduction to Video-Description Project
The goal of the project was to detect activities, objects and background from videos and generate a simple description based on model outputs. Team made use of Transfer Learning with R2plus1D model (trained on Kinetics400 dataset) for activity recognition from videos and fine tuned it for custom  dataset created by our team. Used pre-trained VGG16 model (trained on classes365 dataset) for background detection from image and YOLOv5 for object detection. Finally the output from all these models was processed and given to another open source pre-trained model Key2Text that helped create meaningful sentences from hint words. A Flask app was created to deploy the model on web. Also a Docker image has been built and registered on DockerHub so that project is accessible from anywhere.<br><br>

The final Code has been added to Latest Folder in the repository and running the Flaskapp.py will load all dependencies and web interface will be opened where video file can be uploaded and results will appear on the screen as soon as the models process the files and return the final output in a period of few seconds.<br>

The model has not been added in the repo as the size was exceeding the free limit. So please request for the model weights if you need to run the final app.
