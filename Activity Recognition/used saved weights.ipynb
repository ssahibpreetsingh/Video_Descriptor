{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GgaUcwR5RfzP"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import cv2\n",
        "from torch import nn\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torchvision.transforms as tt\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "from torchvision.models.video import r2plus1d_18, R2Plus1D_18_Weights\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "seed_constant = 27\n",
        "np.random.seed(seed_constant)\n",
        "random.seed(seed_constant)\n",
        "torch.random.seed= seed_constant\n",
        "\n",
        "# Specify the height and width to which each video frame will be resized in our dataset.\n",
        "IMAGE_HEIGHT , IMAGE_WIDTH = 224, 224\n",
        "\n",
        "# Specify the number of frames of a video that will be fed to the model as one sequence.\n",
        "SEQUENCE_LENGTH = 9\n",
        "\n",
        "# CLASSES_LIST\n",
        "CLASSES_LIST=['Person Walking','Person Running',\"Person Exercising\",\"Person Eating\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "model=joblib.load(r\"act_1_joblib.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5k8butmLT-4B"
      },
      "outputs": [],
      "source": [
        "# transforms required by r2plus1d model\n",
        "r21d_trans=tt.Compose([R2Plus1D_18_Weights.KINETICS400_V1.transforms()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3es7vVcNUfRM"
      },
      "outputs": [],
      "source": [
        "# functions to apply transforms to each frame\n",
        "def apply_tansforms(feat):\n",
        "\n",
        "  # list to store transformed frames\n",
        "  feats=[]\n",
        "  for i in range(len(feat)):\n",
        "\n",
        "      #converting to array and reshaping in required format\n",
        "      x=np.transpose(np.array(feat[i]), (0,3,1,2))\n",
        "      # convertin to tensor to apply transforms\n",
        "      a=torch.Tensor(x)\n",
        "      # apply transforms and append to the list\n",
        "      feats.append(r21d_trans(a))\n",
        "  return feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ewwXqiI4A4rO"
      },
      "outputs": [],
      "source": [
        "def val_frames_extraction(video_path,SEQUENCE_LENGTH=9,TIME_SECODNS=3):\n",
        "    '''\n",
        "    This function will extract the required frames from a video after resizing and normalizing them.\n",
        "    Args:\n",
        "        video_path: The path of the video in the disk, whose frames are to be extracted.\n",
        "    Returns:\n",
        "        frames_list: A list containing the resized and normalized frames of the video.\n",
        "    '''\n",
        "\n",
        "    # Declare a list to store video frames.\n",
        "\n",
        "    frames_list = []\n",
        "    vid_list=[]\n",
        "    # Read the Video File using the VideoCapture object.\n",
        "    video_reader = cv2.VideoCapture(video_path)\n",
        "    # Get Frame counts\n",
        "    frame_count=video_reader.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "    # Get FPS\n",
        "    FPS=video_reader.get(cv2.CAP_PROP_FPS)\n",
        "    # Find video length\n",
        "    vid_len=frame_count/FPS\n",
        "    # Finding frames in 3 seconds window\n",
        "    thresh_frames=int(3*FPS)\n",
        "\n",
        "    # Get the total number of frames in the video.\n",
        "    # video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Calculate the the interval after which frames will be added to the list.\n",
        "    # as we need 10 frames for each 3 second video\n",
        "    skip_frames_window = max(int(thresh_frames/SEQUENCE_LENGTH), 1)\n",
        "\n",
        "    # Iterate through the Video Frames.\n",
        "    n_videos=int(vid_len/3)\n",
        "    print(vid_len/3, f\"So dividing into {n_videos} sub videos\")\n",
        "    for i in range(n_videos):\n",
        "      count=i*SEQUENCE_LENGTH\n",
        "      for frame_counter in range(SEQUENCE_LENGTH):\n",
        "          # Set the current frame position of the video.\n",
        "          video_reader.set(cv2.CAP_PROP_POS_FRAMES, (count+frame_counter) * skip_frames_window)\n",
        "\n",
        "          # Reading the frame from the video.\n",
        "          success, frame = video_reader.read()\n",
        "\n",
        "          # Check if Video frame is not successfully read then break the loop\n",
        "          if not success:\n",
        "              break\n",
        "\n",
        "          # Resize the Frame to fixed height and width.\n",
        "          resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
        "\n",
        "          # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1\n",
        "          normalized_frame = resized_frame / 255\n",
        "\n",
        "          # Append the normalized frame into the frames list\n",
        "          frames_list.append(normalized_frame)\n",
        "          # print(\"SEQ Length: \",SEQUENCE_LENGTH)\n",
        "          # print(\"Frame count: \",frame_counter)\n",
        "          # print(\"Len frame list: \",len(frames_list))\n",
        "          # print(\"-\"*50)\n",
        "\n",
        "      vid_list.append(frames_list)\n",
        "      frames_list =[]\n",
        "\n",
        "    # Release the VideoCapture object.\n",
        "    video_reader.release()\n",
        "\n",
        "    # Return the frames list.\n",
        "    return vid_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kd3ajww4A4oo"
      },
      "outputs": [],
      "source": [
        "\n",
        "def preprocess_pred_1(video_file_path):\n",
        "    # video_file_path = \"/content/run_test1.mp4\"\n",
        "    features=[]\n",
        "            # Extract the frames of the video file.\n",
        "    vid_list = val_frames_extraction(video_file_path)\n",
        "            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.\n",
        "            # So ignore the vides having frames less than the SEQUENCE_LENGTH.\n",
        "    # for frames in vid_list:\n",
        "    #   if len(frames) == SEQUENCE_LENGTH:\n",
        "\n",
        "    #       # Append the data to their repective lists.\n",
        "    #       features.append(frames)\n",
        "    #   else:\n",
        "    #       print(len(frames))\n",
        "\n",
        "    # Converting the sub list to numpy arrays\n",
        "\n",
        "    for i in range(len(vid_list)):\n",
        "      vid_list[i]=np.asarray([vid_list[i]])\n",
        "\n",
        "      vid_list[i]=apply_tansforms(vid_list[i])[0]\n",
        "\n",
        "    # Return the frames, class index, and video file path.\n",
        "    return vid_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucKELAg_A8BS",
        "outputId": "297554a4-3b62-422f-fe6c-1b390ca7caa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.8666666666666667 So dividing into 3 sub videos\n"
          ]
        }
      ],
      "source": [
        "feat_1=preprocess_pred_1(\"D:\\cdac\\Project\\walktest.mp4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LhInqGToBD8j"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    y_preds=[]\n",
        "    y_probas=[]\n",
        "    for i in feat_1:\n",
        "      pred=model(i.unsqueeze(0))\n",
        "      # print(pred[0])\n",
        "      probas=(F.softmax(pred[0],dim=0))\n",
        "      pred=torch.argmax(pred, dim = 1).to(\"cpu\").numpy()\n",
        "      y_preds.append(pred)\n",
        "      y_probas.append(probas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-feaW9h5f8f",
        "outputId": "1b59f378-8e41-4423-d061-540f4d0d67e9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(y_probas[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVKgCs6vBSwi",
        "outputId": "5c5d0784-a766-4788-9ebf-4736a88aaacf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Person Walking', 'Person Running', 'Person Exercising', 'Person Eating']\n",
            "tensor([0.6264, 0.1427, 0.1674, 0.0635]) Person Walking\n",
            "tensor([0.7225, 0.2290, 0.0368, 0.0117]) Person Walking\n",
            "tensor([0.7571, 0.1468, 0.0830, 0.0131]) Person Walking\n"
          ]
        }
      ],
      "source": [
        "print(CLASSES_LIST)\n",
        "class_predictions=[CLASSES_LIST[i[0]] for i in y_preds]\n",
        "for i,j in zip(y_probas,class_predictions):\n",
        "  print(i,j)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
