{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "e3aP8p4iHqkh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import cv2\n",
        "from torch import nn\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torchvision.transforms as tt\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "from torchvision.models.video import r2plus1d_18, R2Plus1D_18_Weights\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading data"
      ],
      "metadata": {
        "id": "DsZ_gt19XLgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device=\"cuda\""
      ],
      "metadata": {
        "id": "KuVWL83tfj_d"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_constant = 27\n",
        "np.random.seed(seed_constant)\n",
        "random.seed(seed_constant)\n",
        "torch.random.seed= seed_constant\n",
        "\n",
        "BATCH_SIZE=10\n",
        "\n",
        "# Specify the height and width to which each video frame will be resized in our dataset.\n",
        "IMAGE_HEIGHT , IMAGE_WIDTH = 224,224\n",
        "\n",
        "# Specify the number of frames of a video that will be fed to the model as one sequence.\n",
        "SEQUENCE_LENGTH = 20\n",
        "\n",
        "# Specify the directory containing the UCF50 dataset.\n",
        "DATASET_DIR = \"UCF50\"\n",
        "\n",
        "# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.\n",
        "CLASSES_LIST = [\"Diving\", \"Fencing\", \"GolfSwing\", \"PlayingGuitar\",\"HighJump\",\n",
        "                \"HorseRiding\", \"Swing\", \"Punch\",\"PlayingViolin\",\"JumpRope\",\n",
        "                \"Basketball\",\"Shooting\"]\n"
      ],
      "metadata": {
        "id": "NqZ7lqu8XLWc"
      },
      "execution_count": 292,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "# Downlaod the UCF50 Dataset\n",
        "!wget --no-check-certificate https://www.crcv.ucf.edu/data/UCF50.rar\n",
        "\n",
        "#Extract the Dataset\n",
        "!unrar x UCF50.rar"
      ],
      "metadata": {
        "id": "dtQtMI-dXLTM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all_classes_names = os.listdir('UCF50')\n",
        "# all_classes_names"
      ],
      "metadata": {
        "id": "FlqU1b1jwToq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transforms required by r2plus1d model\n",
        "r21d_trans=tt.Compose([R2Plus1D_18_Weights.KINETICS400_V1.transforms()])"
      ],
      "metadata": {
        "id": "hfutiFK4dZab"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# functions to apply transforms to each frame\n",
        "def apply_tansforms(feat):\n",
        "\n",
        "  # list to store transformed frames\n",
        "  feats=[]\n",
        "  for i in range(len(feat)):\n",
        "\n",
        "      #converting to array and reshaping in required format\n",
        "      x=np.transpose(np.array(feat[i]), (0,3,1,2))\n",
        "      # convertin to tensor to apply transforms\n",
        "      a=torch.Tensor(x)\n",
        "      # apply transforms and append to the list\n",
        "      feats.append(r21d_trans(a))\n",
        "  return feats"
      ],
      "metadata": {
        "id": "CsKx_mWckBmH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# keeping default weights i.e. for Kinetics dataset\n",
        "weights=R2Plus1D_18_Weights.DEFAULT\n",
        "# Initializing the model\n",
        "r21d=r2plus1d_18(weights=weights,progress=True)\n",
        "# getting number of input features in the last layer\n",
        "r21d.fc\n"
      ],
      "metadata": {
        "id": "L8tLFniEKZOC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a87bb808-45e2-44ce-da70-2dacdf4fbcaa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/r2plus1d_18-91a641e6.pth\" to /root/.cache/torch/hub/checkpoints/r2plus1d_18-91a641e6.pth\n",
            "100%|██████████| 120M/120M [00:01<00:00, 82.5MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=512, out_features=400, bias=True)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# putting model on gpu\n",
        "model=r21d.to(device)"
      ],
      "metadata": {
        "id": "F-PxcCoJ1iem"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predictions"
      ],
      "metadata": {
        "id": "WNuC_LLO5TZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def val_frames_extraction(video_path,SEQUENCE_LENGTH=18,TIME_SECODNS=3):\n",
        "    '''\n",
        "    This function will extract the required frames from a video after resizing and normalizing them.\n",
        "    Args:\n",
        "        video_path: The path of the video in the disk, whose frames are to be extracted.\n",
        "        SEQUENCE_LENGTH: Nu,ber of frames per sub-video\n",
        "        TIME_SECODNS: video to be divided into sub-videos of what duration\n",
        "    Returns:\n",
        "        vid_list: A list of list of sub-videos containing the resized and normalized frames.\n",
        "    '''\n",
        "\n",
        "    # Declare a list to store sub-videos and their frames.\n",
        "    vid_list=[]\n",
        "    # Read the Video File using the VideoCapture object.\n",
        "    video_reader = cv2.VideoCapture(video_path)\n",
        "    # Get Frame counts\n",
        "    frame_count=video_reader.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "    # Get FPS\n",
        "    FPS=video_reader.get(cv2.CAP_PROP_FPS)\n",
        "    # Find video length\n",
        "    vid_len=frame_count/FPS\n",
        "    # Finding frames in 3 seconds window\n",
        "    thresh_frames=int(3*FPS)\n",
        "\n",
        "    # Calculate the the interval after which frames will be added to the list.\n",
        "    skip_frames_window = int(thresh_frames/SEQUENCE_LENGTH)\n",
        "\n",
        "    # Iterate through the Video Frames.\n",
        "    n_videos=int(vid_len/3)\n",
        "    print(\"Print division by three gives : \",round(vid_len/3,2),f\"so dividing the full video into {n_videos} sub-videos\")\n",
        "\n",
        "    # Loop to dividide videos into subvideos and append to vid_list\n",
        "    for i in range(n_videos):\n",
        "\n",
        "      # counter for skipping window\n",
        "      count=i*SEQUENCE_LENGTH\n",
        "\n",
        "      # Initialize a list to store frames of sub videos\n",
        "      frames_list=[]\n",
        "\n",
        "      # split each sub video into frames equal to sequence length\n",
        "      for frame_counter in range(SEQUENCE_LENGTH):\n",
        "\n",
        "          # Set the current frame position of the video. and keep skipping to required frames\n",
        "          video_reader.set(cv2.CAP_PROP_POS_FRAMES, (count + frame_counter) * skip_frames_window)\n",
        "\n",
        "          # Reading the frame from the video.\n",
        "          success, frame = video_reader.read()\n",
        "\n",
        "          # Check if Video frame is not successfully read then break the loop\n",
        "          if not success:\n",
        "              break\n",
        "\n",
        "          # Resize the Frame to fixed height and width.\n",
        "          resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
        "\n",
        "          # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1\n",
        "          normalized_frame = resized_frame / 255\n",
        "\n",
        "          # Append the normalized frame into the frames list\n",
        "          frames_list.append(normalized_frame)\n",
        "\n",
        "      # append the list of collected frames from sub-video to vid_list\n",
        "      vid_list.append(frames_list)\n",
        "\n",
        "    # Release the VideoCapture object.\n",
        "    video_reader.release()\n",
        "\n",
        "    # Return the frames list.\n",
        "    return vid_list"
      ],
      "metadata": {
        "id": "8iLOKmvRK16b"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def preprocess_pred(video_file_path=\"/content/video (1080p).mp4\",SEQUENCE_LENGTH=6):\n",
        "    '''\n",
        "    This function will call extract_frames function and apply transforms on each sub-video\n",
        "    Args:\n",
        "        video_file_path: The path of the video in the disk, whose frames are to be extracted.\n",
        "    Returns:\n",
        "        vid_list: A list of transformed sub-videos containing the required preprocessed frames.\n",
        "    '''\n",
        "\n",
        "    # Extract the list of sub-videos and required frames.\n",
        "    vid_list = val_frames_extraction(video_file_path,SEQUENCE_LENGTH=SEQUENCE_LENGTH)\n",
        "\n",
        "    #loop over each sub_video and apply required transformation\n",
        "    for i in range(len(vid_list)):\n",
        "\n",
        "      #converting to numpy array\n",
        "      vid_list[i]=np.asarray([vid_list[i]])\n",
        "      # applying the r21d's specific transformations\n",
        "      vid_list[i]=apply_tansforms(vid_list[i])[0]\n",
        "\n",
        "    # Return processed vid_list\n",
        "    return vid_list"
      ],
      "metadata": {
        "id": "YjxiNleUQ9aB"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "df=pd.read_csv(\"/content/kinetics_400_labels.csv\")\n",
        "class_names=np.array(df[\"name\"])"
      ],
      "metadata": {
        "id": "PCLn4_5HJec8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calling preprocess_pred()\n",
        "video_list=preprocess_pred(\"/content/UCF50/PlayingGuitar/v_PlayingGuitar_g01_c01.avi\",18)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILrvYeccjslK",
        "outputId": "e3c82483-47fd-485b-c7e4-ea78db64081c"
      },
      "execution_count": 503,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Print division by three gives :  3.33 so dividing the full video into 3 sub-videos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###-------------------\n",
        "# Making Predictions\n",
        "###-------------------\n",
        "\n",
        "with torch.no_grad():\n",
        "    # setting model mode to evaluation\n",
        "    model.eval()\n",
        "    # lists to store predictions and softmax probabilities for each sub-video\n",
        "    y_preds=[]\n",
        "    y_probas=[]\n",
        "\n",
        "    #pass each sub video to model and store the predictions\n",
        "    for i in video_list:\n",
        "      # giving batch of 1 and getting predictions\n",
        "      logits=model(i.unsqueeze(0).to(device))\n",
        "      # getting softmax probabilities\n",
        "      probas=(F.softmax(logits[0],dim=0)).to(\"cpu\")\n",
        "      # getting class with highest logit value\n",
        "      pred=torch.argmax(logits, dim = 1).to(\"cpu\").numpy()\n",
        "      print(pred.shape)\n",
        "      # append the predictions and probabilities to resp. lists\n",
        "      y_preds.append(pred)\n",
        "      y_probas.append(probas)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uohDnuPd7Bhq",
        "outputId": "2e395abe-2154-4c63-9c60-98926533974b"
      },
      "execution_count": 504,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1,)\n",
            "(1,)\n",
            "(1,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_preds,max(y_probas[0])#,max(y_probas[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFxR94jL0Y0f",
        "outputId": "850323c8-20bc-427e-e0f2-132071f8fe0c"
      },
      "execution_count": 505,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([array([232]), array([232]), array([232])], tensor(0.8307))"
            ]
          },
          "metadata": {},
          "execution_count": 505
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for j in range(len(y_probas)):\n",
        "  print(f\"----For sub video {j} --------\")\n",
        "  print(np.sort(y_probas[j])[-3:])\n",
        "  print([class_names[i] for i in np.argsort(y_probas[j])[-3:] ])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIl7Kpzmzlah",
        "outputId": "2f9a25a4-7738-486b-d9e1-181fa10e08f8"
      },
      "execution_count": 506,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----For sub video 0 --------\n",
            "[0.01667413 0.05712632 0.83069927]\n",
            "['finger snapping', 'playing violin', 'playing guitar']\n",
            "----For sub video 1 --------\n",
            "[0.05401805 0.36987865 0.4236617 ]\n",
            "['finger snapping', 'playing violin', 'playing guitar']\n",
            "----For sub video 2 --------\n",
            "[0.02555264 0.09929601 0.77150047]\n",
            "['strumming guitar', 'playing violin', 'playing guitar']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_predictions=[class_names[i] for i in y_preds]\n",
        "for i,j in zip(y_probas,class_predictions):\n",
        "  print(max(i),j)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4iLYM8MiqkR",
        "outputId": "36c0e295-a1c1-453f-a82b-8db6a434076f"
      },
      "execution_count": 482,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.2459) ['passing American football (not in game)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "playing guitar and basket ball\n",
        "both show better results with lower FPS (6-10 sec)\n",
        "\n",
        "# inference:-\n",
        "## seems like the videos which have high speed we need more frames and for slower we require less\n",
        "## for videos with more static or less movement lesser frames work better and videos with lots of movements more FPS needed"
      ],
      "metadata": {
        "id": "9BiYkmU7us_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Swing\n",
        "tried with custom video as well as dataset videos(which are not at all that good)\n",
        "10-12 (88,57) (67,81) sec gives best\n",
        "13 sec onwards model starts getting bit confused  (53,61)  second class is real low for all 10,12,13 secs"
      ],
      "metadata": {
        "id": "14s0i4dCrVXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diving\n",
        "g01_c01\n",
        "confused as needed at 15 sec  - 13 sec fine for 1st pred but 2nd pred 74 hammer throw\n",
        "\n",
        "g03_c04\n",
        "confused as needed at 15 sec  - 13 sec fine for 2nd pred but 1st pred 64 hammer throw  (bcauze person jsut moves up-down on board)"
      ],
      "metadata": {
        "id": "Hq2sRyfenRRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punch - overall 13 sec works fine\n",
        "g01_c01\n",
        "70 - 13 sec best\n",
        "\n",
        "g03_c01\n",
        "90 - 8 sec\n",
        "85 - 10 sec   drop kick 8\n",
        "87 - 12 sec   drop kick 6\n",
        "60 - 13 sec   wrestling 30  - best\n",
        "45 - 14 sec   wrestling 28\n",
        "55 - 15 sec   wrestling 25\n",
        "\n",
        "g03_c01\n",
        "73 - 13 sec  drop kick 23 - best\n",
        "\n"
      ],
      "metadata": {
        "id": "lLiaeU84hnYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "4.\n",
        "35X,30X - 2 sec\n",
        "42,27X  - 3 sec\n",
        "48,47X  - 4 sec\n",
        "90,41X  - 5 sec\n",
        "96,27X  - 6 sec\n",
        "97,39X  - 7 sec\n",
        "99,42X  - 8 sec\n",
        "97,31X  - 9 sec\n",
        "99,35X  - 10 sec\n",
        "97,21  -  11 sec\n",
        "97,28  -  12 sec\n",
        "90,20X  - 13 sec\n",
        "78,13X   - 14 sec\n",
        "95,12X  - 15 sec - best\n",
        "86,12X  - 16 sec\n",
        "93,08X  - 17 sec\n",
        "94,13X  - 18 sec\n",
        "92,09   -19 sec\n",
        "97,19   - 20 sec\n",
        "97,34   - 21 sec\n",
        "95,36   - 22 sec\n",
        "95,36   - 22 sec\n",
        "96,47   - 23 sec\n",
        "93,53   - 24 sec\n",
        "91,54   - 25 sec\n",
        "\n",
        "g02C04:\n",
        "\n",
        "31, 80  -  8 sec  x\n",
        "36x,54 -  10 sec x\n",
        "50X,74 - 12 sec  x\n",
        "77,57X - 13 sec\n",
        "72,42X - 14 sec  - best\n",
        "66,51X - 15 sec\n",
        "72,56X - 16 sec\n",
        "51,44  - 18 sec\n",
        "57,57 - 20 sec\n",
        "\n",
        "60, 33X - 25 sec\n",
        "31,31   - 30 sec"
      ],
      "metadata": {
        "id": "YHt8E8pDrBVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-GDRHcpyOgJI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}