{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "btoeSA8EKn2l"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from cv2 import resize\n",
        "import joblib\n",
        "from vgg16_places_365 import VGG16_Places365\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 --------------------------\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "2 --------------------------\n",
            "1/1 [==============================] - 1s 677ms/step\n",
            "3 --------------------------\n",
            "1/1 [==============================] - 1s 582ms/step\n",
            "4 --------------------------\n",
            "1/1 [==============================] - 1s 501ms/step\n",
            "5 --------------------------\n",
            "1/1 [==============================] - 1s 555ms/step\n",
            "6 --------------------------\n",
            "1/1 [==============================] - 1s 531ms/step\n",
            "7 --------------------------\n",
            "1/1 [==============================] - 1s 514ms/step\n",
            "8 --------------------------\n",
            "1/1 [==============================] - 1s 542ms/step\n",
            "9 --------------------------\n",
            "1/1 [==============================] - 1s 545ms/step\n",
            "10 --------------------------\n",
            "1/1 [==============================] - 1s 664ms/step\n",
            "11 --------------------------\n",
            "1/1 [==============================] - 1s 650ms/step\n",
            "12 --------------------------\n",
            "1/1 [==============================] - 1s 602ms/step\n",
            "13 --------------------------\n",
            "1/1 [==============================] - 1s 539ms/step\n",
            "14 --------------------------\n",
            "1/1 [==============================] - 1s 531ms/step\n",
            "15 --------------------------\n",
            "1/1 [==============================] - 0s 489ms/step\n",
            "16 --------------------------\n",
            "1/1 [==============================] - 1s 525ms/step\n",
            "17 --------------------------\n",
            "1/1 [==============================] - 1s 574ms/step\n",
            "18 --------------------------\n",
            "1/1 [==============================] - 1s 547ms/step\n",
            "19 --------------------------\n",
            "1/1 [==============================] - 0s 482ms/step\n",
            "20 --------------------------\n",
            "1/1 [==============================] - 0s 484ms/step\n",
            "21 --------------------------\n",
            "1/1 [==============================] - 0s 468ms/step\n",
            "22 --------------------------\n",
            "1/1 [==============================] - 0s 464ms/step\n",
            "23 --------------------------\n",
            "1/1 [==============================] - 1s 521ms/step\n",
            "24 --------------------------\n",
            "1/1 [==============================] - 0s 497ms/step\n",
            "25 --------------------------\n",
            "1/1 [==============================] - 0s 499ms/step\n",
            "26 --------------------------\n",
            "1/1 [==============================] - 0s 491ms/step\n",
            "27 --------------------------\n",
            "1/1 [==============================] - 0s 485ms/step\n"
          ]
        }
      ],
      "source": [
        "video_path = r\"D:\\\\cdac\\\\Project\\\\walktest.mp4\"\n",
        "try:\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Get Frame counts\n",
        "    frame_count=cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "    # Get FPS\n",
        "    FPS=cap.get(cv2.CAP_PROP_FPS)\n",
        "    # skip window\n",
        "    skip_window=int(3*FPS//9)\n",
        "\n",
        "    # video len\n",
        "    vid_len=frame_count/FPS\n",
        "    thresh=int((vid_len//3)*3)\n",
        "    # print(frame_count,thresh)\n",
        "\n",
        "\n",
        "    # Open a text file for storing object names\n",
        "    text_file = open(r\"detected_background.txt\", \"w\")\n",
        "    \n",
        "    fcount=0\n",
        "    while fcount<(thresh*3):\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, fcount * skip_window)\n",
        "        fcount+=1\n",
        "        print(fcount,\"--------------------------\")\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        resized_frame = cv2.resize(frame, (224,224))\n",
        "        resized_frame=np.expand_dims(resized_frame, 0)\n",
        "        preds = model1.predict(resized_frame)[0]\n",
        "        conf=max(preds)\n",
        "        top_preds = np.argsort(preds)[::-1][0]\n",
        "        \n",
        "\n",
        "        text_file.write(f\"Label: {top_preds}   confidence: {conf}\\n\")\n",
        "        if  fcount%thresh==0:\n",
        "            text_file.write(\"----------------------------\\n\") \n",
        "        # Write the frame to the output video\n",
        "        \n",
        "except Exception as e:\n",
        "    print(e)\n",
        "finally:\n",
        "    # Release video capture and writer, and close the text file\n",
        "    cap.release()\n",
        "    # video_writer.release()\n",
        "    text_file.close()\n",
        "\n",
        "    cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = VGG16_Places365(weights='places')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load the class label\n",
        "file_name = 'categories_places365.txt'\n",
        "classes = list()\n",
        "with open(file_name) as class_file:\n",
        "    for line in class_file:\n",
        "        classes.append(line.strip().split(' ')[0][3:])\n",
        "classes = tuple(classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIpVSMDwPuBI",
        "outputId": "025b6c1d-df95-4c8c-b43a-e3ccf2109f60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 7s 7s/step\n"
          ]
        }
      ],
      "source": [
        "TEST_IMAGE_URL = 'corridor.jpg'\n",
        "\n",
        "image = Image.open(TEST_IMAGE_URL)\n",
        "image = np.array(image, dtype=np.uint8)\n",
        "image = resize(image, (224, 224))\n",
        "image = np.expand_dims(image, 0)\n",
        "\n",
        "predictions_to_return = 5\n",
        "preds = model.predict(image)[0]\n",
        "top_preds = np.argsort(preds)[::-1][0:predictions_to_return]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "conf=sorted(preds)[-5:][::-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkEWt2noKWLr",
        "outputId": "57091afa-5d6e-40b9-d4ba-d082d2319037"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--SCENE CATEGORIES:\n",
            "corridor 0.8336969\n",
            "elevator_lobby 0.08859836\n",
            "lobby 0.03031714\n",
            "atrium/public 0.017264638\n",
            "clean_room 0.011277142\n"
          ]
        }
      ],
      "source": [
        "print('--SCENE CATEGORIES:')\n",
        "# output the prediction\n",
        "for i in range(0, 5):\n",
        "    print(classes[top_preds[i]],conf[i])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
